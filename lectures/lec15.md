# ğŸ“Œ Lecture 15 â€” Stateful Applications & Observability: The Complete Picture

> ğŸ¯ **From stateless simplicity to production-ready stateful workloads with full observability**

---

## ğŸ“ Slide 1 â€“ ğŸš€ The Final Pieces of Production Kubernetes

We've deployed applications, managed configs, and implemented GitOps. Two challenges remain:

* ğŸ—„ï¸ **Stateful apps:** Databases, message queues, caches â€” they need identity and stable storage
* ğŸ“Š **Observability:** If you can't see it, you can't fix it

```mermaid
flowchart LR
  A[ğŸ“¦ Stateless Apps] --> B[ğŸ—„ï¸ StatefulSets]
  B --> C[ğŸ“Š Monitoring]
  C --> D[ğŸ”” Alerting]
  D --> E[ğŸ’ Production Ready]
```

> ğŸ¯ **Goal:** Master stateful workloads and comprehensive cluster observability

---

## ğŸ“ Slide 2 â€“ ğŸ“š Learning Outcomes

By the end of this lecture, you will:

| # | ğŸ¯ Outcome |
|---|-----------|
| 1 | âœ… Understand when to use **StatefulSets** vs Deployments |
| 2 | âœ… Implement **headless services** for pod discovery |
| 3 | âœ… Configure **VolumeClaimTemplates** for per-pod storage |
| 4 | âœ… Deploy **Prometheus** for metrics collection |
| 5 | âœ… Create **Grafana dashboards** for visualization |
| 6 | âœ… Set up **alerting** for proactive incident response |

---

## ğŸ“ Slide 3 â€“ ğŸ—ºï¸ Lecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  SECTION 0: Introduction                    (Slides 1-4)   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“ PRE QUIZ                                (Slide 5)      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  SECTION 1: Stateful Workload Challenges    (Slides 6-10)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  SECTION 2: StatefulSets Deep Dive          (Slides 11-18) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“ MID QUIZ                                (Slide 19)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  SECTION 3: Observability Fundamentals      (Slides 20-28) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  SECTION 4: Production Monitoring           (Slides 29-36) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“ POST QUIZ                               (Slide 37)     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  FINAL: What's Next                         (Slide 38)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Slide 4 â€“ ğŸ¤” The Big Question

> ğŸ’¬ *"You can't manage what you can't measure."*
> â€” Peter Drucker

**Consider these scenarios:**

* ğŸ—„ï¸ **Database cluster:** Pods need stable identity for replication
* ğŸ“Š **3 AM alert:** Is the app slow, or is it the database?
* ğŸ” **Debugging:** "What changed in the last hour?"
* ğŸ”® **Capacity planning:** "Will we run out of storage next month?"

> ğŸ¤” **Think:** How do you know your system is healthy right now?

---

## ğŸ“ Slide 5 â€“ ğŸ“ QUIZ â€” DEVOPS_L15_PRE

---

## ğŸ“ Slide 6 â€“ âš ï¸ Section 1: Why Stateless Isn't Always Enough

**Deployments (Stateless) characteristics:**

```mermaid
flowchart TD
  A[ğŸ“¦ Deployment] --> B[ğŸ“¦ Pod-abc123]
  A --> C[ğŸ“¦ Pod-def456]
  A --> D[ğŸ“¦ Pod-ghi789]

  E[Pods are interchangeable]
  F[Random names]
  G[Any pod can be replaced]
```

* âœ… **Great for:** Web servers, API services, workers
* âŒ **Problem for:** Databases, message queues, distributed systems

---

## ğŸ“ Slide 7 â€“ ğŸ”¥ Pain Point 1: Pod Identity

**Stateful apps need stable identity:**

| ğŸ—„ï¸ Application | ğŸ”‘ Why Identity Matters |
|---------------|------------------------|
| **PostgreSQL** | Primary/replica must know who is who |
| **MongoDB** | Replica set members have specific roles |
| **Kafka** | Brokers identified by stable IDs |
| **Redis Cluster** | Nodes need persistent slots |
| **Elasticsearch** | Nodes join cluster by name |

**With Deployment:**
```
pod-abc123 â†’ restarts â†’ pod-xyz789 (new name!)
```

**The problem:** Other pods can't find it anymore

---

## ğŸ“ Slide 8 â€“ ğŸ”¥ Pain Point 2: Storage Persistence

**Scenario:** 3-node MongoDB replica set

```mermaid
flowchart TD
  A[ğŸ“¦ Deployment] --> B[ğŸ“¦ Pod 1]
  A --> C[ğŸ“¦ Pod 2]
  A --> D[ğŸ“¦ Pod 3]

  E[ğŸ’¾ Shared PVC] --> B
  E --> C
  E --> D

  F[ğŸ˜± Problem: All pods fighting for same storage!]
```

**What we need:**
* ğŸ“¦ Pod 1 â†’ ğŸ’¾ Volume 1 (its own data)
* ğŸ“¦ Pod 2 â†’ ğŸ’¾ Volume 2 (its own data)
* ğŸ“¦ Pod 3 â†’ ğŸ’¾ Volume 3 (its own data)

---

## ğŸ“ Slide 9 â€“ ğŸ”¥ Pain Point 3: Ordered Operations

**Database cluster startup order matters:**

```mermaid
flowchart LR
  A[ğŸ¥‡ Primary starts] --> B[ğŸ¥ˆ Replica 1 joins]
  B --> C[ğŸ¥‰ Replica 2 joins]
```

**Deployment behavior:** All pods start simultaneously
* ğŸ˜± Race condition: Who is primary?
* ğŸ’¥ Data corruption risk

**Shutdown order matters too:**
* ğŸ¥‰ Replicas drain first
* ğŸ¥‡ Primary shuts down last

---

## ğŸ“ Slide 10 â€“ ğŸ“Š Deployment vs StatefulSet

| ğŸ“‹ Aspect | ğŸ“¦ Deployment | ğŸ—„ï¸ StatefulSet |
|----------|--------------|----------------|
| Pod names | Random suffix | Stable ordinal (app-0, app-1) |
| Storage | Shared or none | Per-pod PVCs |
| Scaling | Parallel | Sequential |
| Updates | Rolling (parallel) | Rolling (sequential) |
| Network identity | Via Service | Stable DNS per pod |
| Use case | Stateless apps | Stateful apps |

---

## ğŸ“ Slide 11 â€“ âœ… Section 2: StatefulSets to the Rescue

**StatefulSet provides:**

```mermaid
flowchart TD
  A[ğŸ—„ï¸ StatefulSet] --> B[ğŸ“¦ app-0]
  A --> C[ğŸ“¦ app-1]
  A --> D[ğŸ“¦ app-2]

  B --> E[ğŸ’¾ data-app-0]
  C --> F[ğŸ’¾ data-app-1]
  D --> G[ğŸ’¾ data-app-2]

  H[ğŸŒ Headless Service] --> B
  H --> C
  H --> D
```

* ğŸ”¢ **Stable, unique network IDs:** `app-0`, `app-1`, `app-2`
* ğŸ’¾ **Stable, persistent storage:** Each pod gets its own PVC
* ğŸ“Š **Ordered deployment:** `app-0` first, then `app-1`, then `app-2`
* ğŸ”„ **Ordered termination:** Reverse order

---

## ğŸ“ Slide 12 â€“ ğŸŒ Headless Services Explained

**Regular Service vs Headless Service:**

```yaml
# Regular Service - Load balances
apiVersion: v1
kind: Service
metadata:
  name: my-app
spec:
  clusterIP: 10.0.0.100  # Gets an IP
  ports:
    - port: 80
---
# Headless Service - Direct pod access
apiVersion: v1
kind: Service
metadata:
  name: my-app-headless
spec:
  clusterIP: None  # No IP assigned!
  ports:
    - port: 80
```

**DNS resolution:**
* **Regular:** `my-app.namespace.svc` â†’ `10.0.0.100`
* **Headless:** `my-app-headless.namespace.svc` â†’ `10.1.2.3, 10.1.2.4, 10.1.2.5` (all pod IPs)

---

## ğŸ“ Slide 13 â€“ ğŸ”— Pod DNS in StatefulSets

**Each pod gets a stable DNS name:**

```
<pod-name>.<service-name>.<namespace>.svc.cluster.local
```

**Example with MongoDB:**
```
mongodb-0.mongodb-headless.default.svc.cluster.local
mongodb-1.mongodb-headless.default.svc.cluster.local
mongodb-2.mongodb-headless.default.svc.cluster.local
```

```mermaid
flowchart LR
  A[ğŸ” DNS Query: mongodb-0.mongodb-headless] --> B[ğŸ“¦ mongodb-0]
  C[ğŸ” DNS Query: mongodb-1.mongodb-headless] --> D[ğŸ“¦ mongodb-1]
```

* âœ… Other apps can connect to specific pods
* âœ… Pods can discover each other by name
* âœ… Names stay the same even after restart

---

## ğŸ“ Slide 14 â€“ ğŸ’¾ VolumeClaimTemplates

**Automatic PVC creation per pod:**

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mongodb
spec:
  serviceName: mongodb-headless
  replicas: 3
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes: ["ReadWriteOnce"]
        storageClassName: standard
        resources:
          requests:
            storage: 10Gi
```

**Result:**
```
PVC: data-mongodb-0 â†’ bound to mongodb-0
PVC: data-mongodb-1 â†’ bound to mongodb-1
PVC: data-mongodb-2 â†’ bound to mongodb-2
```

---

## ğŸ“ Slide 15 â€“ ğŸ”„ Scaling Behavior

**Scale up (sequential):**
```mermaid
flowchart LR
  A[app-0 ready] --> B[app-1 starts]
  B --> C[app-1 ready]
  C --> D[app-2 starts]
```

**Scale down (reverse order):**
```mermaid
flowchart LR
  A[app-2 terminates] --> B[app-2 gone]
  B --> C[app-1 terminates]
  C --> D[app-1 gone]
```

**Key points:**
* ğŸ”„ Next pod only starts when previous is **Ready**
* ğŸ—‘ï¸ Scaling down starts from highest ordinal
* ğŸ’¾ PVCs are **NOT deleted** on scale down (data preserved)

---

## ğŸ“ Slide 16 â€“ ğŸ”„ Update Strategies

**RollingUpdate (default):**
```yaml
spec:
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 0  # Update all pods
```

**Partition for canary:**
```yaml
rollingUpdate:
  partition: 2  # Only update pods >= 2
```

```mermaid
flowchart LR
  A[app-0: v1] --> B[app-1: v1]
  B --> C[app-2: v2 - canary]
```

* âœ… Test new version on highest ordinal first
* âœ… Gradually lower partition to update more pods

---

## ğŸ“ Slide 17 â€“ ğŸ—„ï¸ Complete StatefulSet Example

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mongodb
spec:
  selector:
    matchLabels:
      app: mongodb
  serviceName: mongodb-headless
  replicas: 3
  template:
    metadata:
      labels:
        app: mongodb
    spec:
      containers:
        - name: mongodb
          image: mongo:7
          ports:
            - containerPort: 27017
          volumeMounts:
            - name: data
              mountPath: /data/db
  volumeClaimTemplates:
    - metadata:
        name: data
      spec:
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: 10Gi
```

---

## ğŸ“ Slide 18 â€“ âš ï¸ StatefulSet Gotchas

| âš ï¸ Gotcha | ğŸ“ Solution |
|----------|------------|
| PVCs not deleted on scale down | Manual deletion if needed |
| Pod stuck in Pending | Check PVC binding, storage class |
| Headless service required | Must create before StatefulSet |
| Slow scaling | Increase `podManagementPolicy: Parallel` |
| Data loss on PVC deletion | Use `reclaimPolicy: Retain` |

**Important:** StatefulSets are **more complex** than Deployments. Use only when needed!

---

## ğŸ“ Slide 19 â€“ ğŸ“ QUIZ â€” DEVOPS_L15_MID

---

## ğŸ“ Slide 20 â€“ ğŸ“Š Section 3: The Three Pillars of Observability

**Observability = Understanding system behavior from outputs**

```mermaid
flowchart TD
  A[ğŸ“Š Metrics] --> D[ğŸ” Observability]
  B[ğŸ“ Logs] --> D
  C[ğŸ”— Traces] --> D
  D --> E[ğŸ’¡ Understanding]
```

| ğŸ“Š Pillar | ğŸ“ What It Answers | ğŸ› ï¸ Tools |
|----------|-------------------|----------|
| **Metrics** | What is happening? (numbers) | Prometheus, Grafana |
| **Logs** | What happened? (events) | Loki, ELK |
| **Traces** | Where did it happen? (requests) | Jaeger, Zipkin |

---

## ğŸ“ Slide 21 â€“ ğŸ“ˆ Prometheus: The Metrics Foundation

**What is Prometheus?**

* ğŸ“Š Time-series database for metrics
* ğŸ” Pull-based metric collection
* ğŸ“ Powerful query language (PromQL)
* ğŸ”” Built-in alerting

```mermaid
flowchart LR
  A[ğŸ“¦ App with /metrics] --> |Pull| B[ğŸ“Š Prometheus]
  C[ğŸ“¦ Another App] --> |Pull| B
  B --> D[ğŸ“ˆ Grafana]
  B --> E[ğŸ”” Alertmanager]
```

---

## ğŸ“ Slide 22 â€“ ğŸ¯ Prometheus Metric Types

| ğŸ“Š Type | ğŸ“ Description | ğŸ¯ Example |
|--------|---------------|-----------|
| **Counter** | Only goes up | Total requests, errors |
| **Gauge** | Can go up/down | Temperature, queue size |
| **Histogram** | Distribution of values | Request latency |
| **Summary** | Similar to histogram | Quantiles |

**Example metrics:**
```promql
# Counter - total HTTP requests
http_requests_total{method="GET", status="200"}

# Gauge - current memory usage
node_memory_MemAvailable_bytes

# Histogram - request duration
http_request_duration_seconds_bucket{le="0.5"}
```

---

## ğŸ“ Slide 23 â€“ ğŸ” PromQL Basics

**Query examples:**

```promql
# Current value
up{job="kubernetes-pods"}

# Rate of change (per second over 5m)
rate(http_requests_total[5m])

# Error rate percentage
sum(rate(http_requests_total{status=~"5.."}[5m])) /
sum(rate(http_requests_total[5m])) * 100

# 99th percentile latency
histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))

# Top 5 pods by CPU
topk(5, sum by (pod) (rate(container_cpu_usage_seconds_total[5m])))
```

---

## ğŸ“ Slide 24 â€“ ğŸ“¦ kube-prometheus-stack

**All-in-one monitoring solution:**

```mermaid
flowchart TD
  A[ğŸ“¦ kube-prometheus-stack] --> B[ğŸ“Š Prometheus]
  A --> C[ğŸ“ˆ Grafana]
  A --> D[ğŸ”” Alertmanager]
  A --> E[ğŸ“ Node Exporter]
  A --> F[ğŸ“Š kube-state-metrics]
```

**Includes:**
* ğŸ”§ Pre-configured scrape targets
* ğŸ“Š Default dashboards
* ğŸ”” Default alerting rules
* ğŸ“ˆ Grafana with data sources configured

```bash
helm install prometheus prometheus-community/kube-prometheus-stack
```

---

## ğŸ“ Slide 25 â€“ ğŸ“ˆ Grafana Dashboards

**Key Grafana concepts:**

| ğŸ”§ Concept | ğŸ“ Description |
|-----------|---------------|
| **Data Source** | Where data comes from (Prometheus) |
| **Dashboard** | Collection of panels |
| **Panel** | Single visualization |
| **Variable** | Dynamic filters (namespace, pod) |

**Popular pre-built dashboards:**
* ğŸ”¢ **1860:** Node Exporter Full
* ğŸ”¢ **315:** Kubernetes cluster
* ğŸ”¢ **7249:** Kubernetes Pod Resources

---

## ğŸ“ Slide 26 â€“ ğŸ”” Alerting with Alertmanager

**Alert flow:**

```mermaid
flowchart LR
  A[ğŸ“Š Prometheus] --> |Alert fires| B[ğŸ”” Alertmanager]
  B --> |Route| C[ğŸ“§ Email]
  B --> |Route| D[ğŸ’¬ Slack]
  B --> |Route| E[ğŸ“± PagerDuty]
```

**Alert rule example:**
```yaml
groups:
  - name: app-alerts
    rules:
      - alert: HighErrorRate
        expr: |
          sum(rate(http_requests_total{status=~"5.."}[5m])) /
          sum(rate(http_requests_total[5m])) > 0.01
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }}"
```

---

## ğŸ“ Slide 27 â€“ ğŸ“Š The Four Golden Signals

**Google SRE's essential metrics:**

| ğŸ”” Signal | ğŸ“ What to Measure | ğŸ“Š Prometheus Example |
|----------|-------------------|----------------------|
| **Latency** | Response time | `histogram_quantile(0.99, ...)` |
| **Traffic** | Request rate | `rate(http_requests_total[5m])` |
| **Errors** | Failure rate | `rate(http_requests_total{status=~"5.."}[5m])` |
| **Saturation** | Resource usage | `container_memory_usage_bytes / limit` |

> ğŸ’¡ **Tip:** Start by monitoring these four signals for every service

---

## ğŸ“ Slide 28 â€“ ğŸ”§ Init Containers for Dependencies

**Problem:** App starts before database is ready

```yaml
spec:
  initContainers:
    - name: wait-for-db
      image: busybox
      command:
        - sh
        - -c
        - |
          until nc -z postgres-0.postgres-headless 5432; do
            echo "Waiting for database..."
            sleep 2
          done
  containers:
    - name: app
      image: my-app
```

```mermaid
flowchart LR
  A[ğŸš€ Pod Start] --> B[â³ Init Container]
  B --> |DB Ready| C[ğŸ“¦ App Container]
```

---

## ğŸ“ Slide 29 â€“ ğŸ­ Section 4: Production Monitoring Setup

**ServiceMonitor for custom apps:**

```yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: my-app-monitor
spec:
  selector:
    matchLabels:
      app: my-app
  endpoints:
    - port: metrics
      interval: 30s
      path: /metrics
```

**What this does:**
* ğŸ” Tells Prometheus to scrape your app
* ğŸ“Š Collects metrics from `/metrics` endpoint
* â±ï¸ Every 30 seconds

---

## ğŸ“ Slide 30 â€“ ğŸ“Š Monitoring StatefulSets

**Key metrics for stateful apps:**

| ğŸ“Š Metric | ğŸ“ Why Important |
|----------|------------------|
| `kubelet_volume_stats_used_bytes` | Disk usage per PVC |
| `kube_statefulset_replicas` | Expected vs actual replicas |
| `kube_statefulset_status_replicas_ready` | Healthy replicas |
| App-specific metrics | Replication lag, connections |

**Alert example:**
```yaml
- alert: StatefulSetNotReady
  expr: |
    kube_statefulset_status_replicas_ready /
    kube_statefulset_replicas < 1
  for: 5m
  labels:
    severity: warning
```

---

## ğŸ“ Slide 31 â€“ ğŸ“ˆ Resource Monitoring

**CPU and Memory queries:**

```promql
# CPU usage percentage
sum(rate(container_cpu_usage_seconds_total{pod=~"my-app.*"}[5m])) /
sum(kube_pod_container_resource_limits{resource="cpu", pod=~"my-app.*"}) * 100

# Memory usage percentage
sum(container_memory_working_set_bytes{pod=~"my-app.*"}) /
sum(kube_pod_container_resource_limits{resource="memory", pod=~"my-app.*"}) * 100
```

**Capacity planning alerts:**
```yaml
- alert: HighMemoryUsage
  expr: |
    sum(container_memory_working_set_bytes) by (pod) /
    sum(kube_pod_container_resource_limits{resource="memory"}) by (pod) > 0.8
  for: 15m
```

---

## ğŸ“ Slide 32 â€“ ğŸ”” Alert Fatigue Prevention

**Problem:** Too many alerts = ignored alerts

| âŒ Bad Practice | âœ… Better Approach |
|----------------|-------------------|
| Alert on every metric | Alert on symptoms, not causes |
| No severity levels | Critical, warning, info tiers |
| Alert immediately | Use `for` duration |
| Generic messages | Actionable descriptions |
| No runbooks | Link to debugging guides |

**Good alert structure:**
```yaml
annotations:
  summary: "High error rate on {{ $labels.service }}"
  description: "Error rate is {{ $value }}% (threshold: 1%)"
  runbook_url: "https://wiki/alerts/high-error-rate"
```

---

## ğŸ“ Slide 33 â€“ ğŸ“Š Dashboard Best Practices

**Effective dashboard layout:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“Š Overview: Key metrics at a glance   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ”´ Errors  â”‚  â±ï¸ Latency â”‚  ğŸ“ˆ Traffic â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ’¾ Resource Usage (CPU, Memory, Disk)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ” Detailed Breakdowns (per pod, etc.) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Tips:**
* ğŸ“Š Start with high-level, drill down for details
* ğŸ¨ Use consistent colors (red = bad)
* ğŸ“ Add descriptions to panels
* ğŸ”— Link related dashboards

---

## ğŸ“ Slide 34 â€“ ğŸ¢ Real-World: Observability at Scale

**Netflix observability approach:**

* ğŸ“Š **Metrics:** Atlas (Prometheus-like, billions of time series)
* ğŸ“ **Logs:** Mantis (real-time stream processing)
* ğŸ”— **Traces:** Edgar (distributed tracing)
* ğŸ”” **Alerts:** Focused on customer impact

**Key lessons:**
* ğŸ¯ Focus on **business metrics** (not just infra)
* ğŸ”„ Automate **remediation** where possible
* ğŸ“ˆ Invest in **dashboards** as a product
* ğŸ‘¥ Make observability **everyone's** job

---

## ğŸ“ Slide 35 â€“ ğŸ¯ Key Takeaways

1. ğŸ—„ï¸ **StatefulSets** provide stable identity and per-pod storage for databases
2. ğŸŒ **Headless services** enable direct pod-to-pod communication
3. ğŸ“Š **Three pillars:** Metrics, Logs, Traces for full observability
4. ğŸ“ˆ **Prometheus + Grafana** is the standard K8s monitoring stack
5. ğŸ”” **Alerts should be actionable** â€” avoid alert fatigue
6. ğŸ¯ **Four Golden Signals:** Latency, Traffic, Errors, Saturation

> ğŸ’¬ *"Observability is not about collecting data, it's about understanding your system."*

---

## ğŸ“ Slide 36 â€“ ğŸ§  Mindset Shift

| ğŸ˜° Old Mindset | ğŸš€ New Mindset |
|---------------|----------------|
| "It's working, don't touch it" | "I can see it's working" |
| "Let's check the logs" | "The dashboard shows the issue" |
| "User reported an error" | "Alert fired before impact" |
| "Database needs restart" | "DB has stable identity, restart is safe" |
| "Collect all the metrics" | "Monitor what matters" |
| "Alert on everything" | "Alert on symptoms, investigate causes" |

> ğŸ¤” **Question:** What's the first dashboard you'd build for your app?

---

## ğŸ“ Slide 37 â€“ ğŸ“ QUIZ â€” DEVOPS_L15_POST

---

## ğŸ“ Slide 38 â€“ ğŸš€ What's Next?

**Coming up: Lecture 16 â€” Beyond Kubernetes**

```mermaid
flowchart LR
  A[â˜¸ï¸ Kubernetes] --> B[âœˆï¸ Fly.io]
  A --> C[ğŸŒ IPFS/4EVERLAND]
  B --> D[ğŸŒ Global Edge]
  C --> E[ğŸ”— Decentralized]
```

* âœˆï¸ **Fly.io:** Edge deployment simplified
* ğŸŒ **IPFS:** Decentralized hosting
* ğŸ¤” **When to use what:** Trade-offs and decisions
* ğŸ¯ **Beyond the cluster:** Alternative deployment models

> ğŸ¯ **Labs 15 & 16:** Convert your app to StatefulSet and set up comprehensive monitoring!

---

## ğŸ“š Resources

**StatefulSets:**
* ğŸ“– [Kubernetes StatefulSets](https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/)
* ğŸ“– [Headless Services](https://kubernetes.io/docs/concepts/services-networking/service/#headless-services)

**Observability:**
* ğŸ“– [Prometheus Docs](https://prometheus.io/docs/)
* ğŸ“– [Grafana Docs](https://grafana.com/docs/)
* ğŸ“– [kube-prometheus-stack](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack)

**Books:**
* ğŸ“• *Observability Engineering* by Charity Majors, et al.
* ğŸ“• *Site Reliability Engineering* by Google
* ğŸ“• *Kubernetes Patterns* by Bilgin Ibryam & Roland HuÃŸ
